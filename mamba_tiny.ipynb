{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install datasets==2.16.0 mamba-ssm==1.1.1 accelerate==0.25.0 huggingface-hub==0.19.4\n",
        "\n",
        "# See: https://www.reddit.com/r/LocalLLaMA/comments/18da1al/an_interactive_demo_for_mambachat/\n",
        "!export LC_ALL=\"en_US.UTF-8\"\n",
        "!export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n",
        "!export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n",
        "!ldconfig /usr/lib64-nvidia"
      ],
      "metadata": {
        "id": "NyFgMceT_wfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tb5OUm2u_jMZ"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "TRAIN_NUM = 50000\n",
        "VAL_NUM = 5000\n",
        "\n",
        "train_split = f\"train[:{TRAIN_NUM}]\"\n",
        "val_split = f\"train[{TRAIN_NUM}:{TRAIN_NUM + VAL_NUM}]\"\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"VietAI/gpt-neo-1.3B-vietnamese-news\")\n",
        "# train_dataset = load_dataset(\"nampdn-ai/tinystories-vietnamese\", split=\"train[:500]\").map(lambda d: tokenizer(d[\"vi\"]))\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
        "train_dataset = load_dataset(\"roneneldan/TinyStories\", split=train_split).map(lambda d: tokenizer(d[\"text\"]))\n",
        "val_dataset = load_dataset(\"roneneldan/TinyStories\", split=val_split).map(lambda d: tokenizer(d[\"text\"]))\n",
        "tokenizer.eos_token = \"<|endoftext|>\"\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from mamba_ssm.models.config_mamba import MambaConfig\n",
        "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n",
        "\n",
        "class CustomModel(MambaLMHeadModel):\n",
        "    def forward(self, input_ids, position_ids=None, inference_params=None, num_last_tokens=0, attention_mask=None):\n",
        "        return super().forward(\n",
        "            input_ids,\n",
        "            position_ids=position_ids, inference_params=inference_params, num_last_tokens=num_last_tokens,\n",
        "        )\n",
        "\n",
        "config = MambaConfig()\n",
        "config.d_model = 256\n",
        "config.n_layer = 16\n",
        "config.vocab_size = tokenizer.vocab_size\n",
        "model = CustomModel(config, device=\"cuda\")\n",
        "print(\"Model's parameters:\", sum(p.numel() for p in model.parameters()))"
      ],
      "metadata": {
        "id": "nYCsArGC-uQo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af8ec767-a8d0-4927-bd34-514c648aae03"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model's parameters: 19876096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# See: https://github.com/havenhq/mamba-chat/blob/bbc9ef6/trainer/mamba_trainer.py\n",
        "class MambaTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        input_ids = inputs.pop(\"input_ids\")\n",
        "        lm_logits = model(input_ids).logits\n",
        "        labels = input_ids.to(lm_logits.device)\n",
        "        shift_logits = lm_logits[:, :-1, :].contiguous()\n",
        "        labels = labels[:, 1:].contiguous()\n",
        "        loss_fct = torch.nn.CrossEntropyLoss()\n",
        "        lm_loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), labels.view(-1))\n",
        "        return (lm_loss, lm_logits) if return_outputs else lm_loss\n",
        "\n",
        "    def save_model(self, output_dir, _internal_call):\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "        torch.save(self.model.state_dict(), f\"{output_dir}/pytorch_model.bin\")\n",
        "        self.tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "trainer = MambaTrainer(\n",
        "    model=model,\n",
        "    args=TrainingArguments(\n",
        "        \"mamba-tiny\",\n",
        "        evaluation_strategy=\"steps\",\n",
        "        gradient_accumulation_steps=100, num_train_epochs=1,\n",
        "        logging_steps=0.05, report_to=\"none\",\n",
        "    ),\n",
        "    train_dataset=train_dataset, eval_dataset=val_dataset, tokenizer=tokenizer,\n",
        ")\n",
        "trainer.can_return_loss = True\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "RpJwpTPPBGZp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        },
        "outputId": "1141e071-b5ec-4a0b-b0c6-573183da9da9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='62' max='62' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [62/62 27:46, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>9.333400</td>\n",
              "      <td>9.599450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>8.765700</td>\n",
              "      <td>9.411439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>8.568400</td>\n",
              "      <td>9.274098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>8.469600</td>\n",
              "      <td>9.144461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>8.352000</td>\n",
              "      <td>9.025208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>8.273500</td>\n",
              "      <td>8.925272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>8.126000</td>\n",
              "      <td>8.844156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>8.074900</td>\n",
              "      <td>8.776877</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>8.006300</td>\n",
              "      <td>8.721031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>7.974100</td>\n",
              "      <td>8.674876</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>7.992500</td>\n",
              "      <td>8.637071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>7.930200</td>\n",
              "      <td>8.606954</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>7.816800</td>\n",
              "      <td>8.584733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>7.907700</td>\n",
              "      <td>8.569792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>7.842300</td>\n",
              "      <td>8.561920</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=62, training_loss=8.215789871831094, metrics={'train_runtime': 1693.4207, 'train_samples_per_second': 29.526, 'train_steps_per_second': 0.037, 'total_flos': 0.0, 'train_loss': 8.215789871831094, 'epoch': 0.99})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# See: https://github.com/state-spaces/mamba/blob/1df0df1/benchmarks/benchmark_generation_mamba_simple.py\n",
        "def complete(model, tokenizer, input_text: str) -> str:\n",
        "    input_tokens = tokenizer(input_text, return_tensors=\"pt\")\n",
        "    input_ids = input_tokens.input_ids.to(\"cuda\")\n",
        "    output_ids = model.generate(input_ids, max_length=100, return_dict_in_generate=True, cg=True)\n",
        "    output_text = tokenizer.batch_decode(output_ids.sequences.tolist())[0]\n",
        "    return output_text"
      ],
      "metadata": {
        "id": "6XAVGddZBNA9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "complete(model, tokenizer, \"Once\")"
      ],
      "metadata": {
        "id": "HWtzjCS2BWXb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "80decb2f-a027-410a-9cd5-ecc01b8b5bdf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Once upon a time, there was a little girl named Lily.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}